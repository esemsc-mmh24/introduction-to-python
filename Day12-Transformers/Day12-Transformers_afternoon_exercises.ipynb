{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YehS8enAmDn"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1YLNtm8gNsviTEnVXzfiby2VMKrc0XzLP\" width=\"500\"/>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koBaqXTQrby5"
      },
      "source": [
        "# **Transformers**\n",
        "\n",
        "#### **Morning contents/agenda**\n",
        "\n",
        "1. What is a Transformer?\n",
        "\n",
        "2. Applications and impact\n",
        "\n",
        "3. Dissecting a Transformer:\n",
        "  - Embeddings\n",
        "  - Positional Encoding\n",
        "  - Self-attention mechanism & multi-head attention\n",
        "  - Dimensions, parallelisation and residual connections\n",
        "  - Masked multi-head attention and other decoder differences\n",
        "  - Regularisation and optimiser\n",
        "\n",
        "4. Visualisation of transformers\n",
        "\n",
        "5. `torch.nn.Transformer`\n",
        "\n",
        "6. Extensions: Vision Transformers (ViTs) and multi-modal Transformers\n",
        "\n",
        "#### **Learning outcomes**\n",
        "\n",
        "1. Understand the transformer architecture based on the self-attention mechanism\n",
        "\n",
        "2. Develop an intuition of what self-attention does by visualising attention maps\n",
        "\n",
        "3. Increase a bit our awareness of the full Deep Learning landscape\n",
        "\n",
        "\\\\\n",
        "\n",
        "#### **Afternoon contents/agenda**\n",
        "\n",
        "1. Explore a pre-trained word embedding\n",
        "\n",
        "2. Implement a multi-headed attention layer\n",
        "\n",
        "3. Build and train a compact transformer\n",
        "\n",
        "#### **Learning outcomes**\n",
        "\n",
        "1. Become familiar with some popular textual datasets and pre-trained embeddings\n",
        "\n",
        "2. Gain hands-on experience on how the self-attention mechanism works in practice\n",
        "\n",
        "3. Understand how transformers are structured and some of the heuristics that make them trainable\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_kYki018eTFJ",
        "outputId": "c9ac609f-2187-489b-dd49-757fdd9eac70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycm\n",
            "  Downloading pycm-4.1-py3-none-any.whl.metadata (49 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting livelossplot\n",
            "  Downloading livelossplot-0.5.5-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting art>=1.8 (from pycm)\n",
            "  Downloading art-6.4-py3-none-any.whl.metadata (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from pycm) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.8.0)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.6.2)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (3.1.4)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.3.1)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (24.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (2.2.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (11.0.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.3.3)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (2024.9.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->livelossplot) (1.16.0)\n",
            "Downloading pycm-4.1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading livelossplot-0.5.5-py3-none-any.whl (22 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading art-6.4-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.6/608.6 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchinfo, art, pycm, livelossplot\n",
            "Successfully installed art-6.4 livelossplot-0.5.5 pycm-4.1 torchinfo-1.8.0\n",
            "Collecting torchtext==0.14.0\n",
            "  Downloading torchtext-0.14.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting torchdata==0.5.0\n",
            "  Downloading torchdata-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.0) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.0) (2.32.3)\n",
            "Collecting torch==1.13.0 (from torchtext==0.14.0)\n",
            "  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.0) (1.26.4)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.0) (2.2.3)\n",
            "Collecting portalocker>=2.0.0 (from torchdata==0.5.0)\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (4.12.2)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.0->torchtext==0.14.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.0->torchtext==0.14.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.0->torchtext==0.14.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.0->torchtext==0.14.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext==0.14.0) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext==0.14.0) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (2024.8.30)\n",
            "Downloading torchtext-0.14.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, torchtext, torchdata\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 portalocker-3.0.0 torch-1.13.0 torchdata-0.5.0 torchtext-0.14.0\n",
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "!pip install pycm livelossplot torchinfo\n",
        "!pip install torchtext==0.14.0 torchdata==0.5.0\n",
        "%pylab inline\n",
        "\n",
        "import random\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from livelossplot import PlotLosses\n",
        "from pycm import *\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from torchtext.vocab import GloVe, vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LI8sNA9feT3H",
        "outputId": "c64cc784-1372-4262-fb6c-acadc80ceba3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda installed! Running on GPU!\n"
          ]
        }
      ],
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = False\n",
        "\n",
        "    return True\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EirUJEpBVvU3"
      },
      "source": [
        "## 1. Explore a pre-trained word embedding\n",
        "\n",
        "### **Word embedding**\n",
        "\n",
        "The first step in any text-processing neural network is to transform text values into numerical values that can be processed by our networks. As we have seen, using embeddings is an efficient way to achieve this for large vocabularies.\n",
        "\n",
        "It is possible to design and train your own embeddings, but it is also possible to use embeddings that have already been trained by others.\n",
        "\n",
        "For this exercise, we are going to be using [GloVe (Global Vectors for Word Representation)](https://nlp.stanford.edu/projects/glove/), which is conveniently pre-packaged as part of `torchtext`. You can find it in the documentation [here](https://pytorch.org/text/stable/vocab.html#glove).\n",
        "\n",
        "Download the GloVe embeddings. I recomment you start with the 6B version with a dimensionality of 300:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9tfFMN6qes1h"
      },
      "outputs": [],
      "source": [
        "# Load GloVe through the torchtext interface\n",
        "glove = GloVe(name='6B', dim=300)  # Download the GloVe embeddings\n",
        "glove_vocab = vocab(glove.stoi)   # Extract the vocabulary\n",
        "\n",
        "# Insert some special characters that are not present in the original vocabulary\n",
        "# '' and ''\n",
        "glove_vocab.insert_token('<unk>', 0)  # Insert <unk> at index 0\n",
        "glove_vocab.insert_token('<pad>', 1)  # Insert <pad> at index 1\n",
        "glove_vocab.set_default_index(0)      # Make index 0 the default for the vocabulary\n",
        "\n",
        "# and add corresponding vectors to the embeddings\n",
        "pretrained_embeddings = glove.vectors  # Extract the embedding vectors\n",
        "pretrained_embeddings = torch.cat((torch.mean(pretrained_embeddings, axis=0, keepdims=True),  # Mean of all vectors for <unk>\n",
        "                                   torch.zeros(1, pretrained_embeddings.shape[1]),            # Zeros for <pad>\n",
        "                                   pretrained_embeddings))                                    # All other vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq5liR3JyepD"
      },
      "source": [
        "To interact with the embeddings, let's create some utility functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sufN2qanes1i"
      },
      "outputs": [],
      "source": [
        "def get_word(word):\n",
        "    \"\"\"\n",
        "    Get the embedding vector for a specific word.\n",
        "    \"\"\"\n",
        "    return glove.vectors[glove.stoi[word]]\n",
        "\n",
        "def closest(vec, n=10):\n",
        "    \"\"\"\n",
        "    Find the closest words for a given vector.\n",
        "    \"\"\"\n",
        "    all_dists = [(w, torch.dist(vec, get_word(w))) for w in glove.itos]\n",
        "    return sorted(all_dists, key=lambda t: t[1])[:n]\n",
        "\n",
        "def print_tuples(tuples):\n",
        "    \"\"\"\n",
        "    Print tuples with formatting.\n",
        "    \"\"\"\n",
        "    for tuple in tuples:\n",
        "        print('(%.4f) %s' % (tuple[1], tuple[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3QtTFv6y2f1"
      },
      "source": [
        "Using the functions above, find the closest vectors in the embedding to the word `dog`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6cg-FZ_shGnc",
        "outputId": "581695cb-7424-4a1f-9d87-ab22b97911bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.0000) dog\n",
            "(4.3599) dogs\n",
            "(5.1959) cat\n",
            "(5.7008) pet\n",
            "(5.8576) puppy\n",
            "(6.2202) hound\n",
            "(6.3971) pets\n",
            "(6.4064) animal\n",
            "(6.4334) __________________________________\n",
            "(6.4496) canine\n"
          ]
        }
      ],
      "source": [
        "print_tuples(closest(get_word(\"dog\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsSMTzLBzdek"
      },
      "source": [
        "Perform the following operations on embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O-PSSdE_gjrC",
        "outputId": "b27a0601-d344-4db9-fdfb-e24861b81c7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5.9553) woman\n",
            "(6.9806) girl\n",
            "(7.2121) person\n",
            "(7.4310) teenager\n",
            "(7.4553) she\n"
          ]
        }
      ],
      "source": [
        "# 'man' - 'king' + 'queen' = ?\n",
        "closest_words = closest(get_word(\"man\") - get_word(\"king\") + get_word(\"queen\"))\n",
        "\n",
        "# Filter out original words\n",
        "closest_words = [t for t in closest_words if t[0] not in ['king', 'man', 'queen']]\n",
        "\n",
        "# Print result\n",
        "print_tuples(closest_words[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuTr7JZxz9Qo"
      },
      "source": [
        "### **AG_NEWS dataset**\n",
        "\n",
        "As part of the library `torchtext`, there are also a number of text datasets available to us. In this exercise, we are going to be using the [`AG_NEWS`](https://pytorch.org/text/stable/datasets.html#ag-news) dataset. This dataset contains extracts of news articles from around the world, labeled according to the type of news: `World`, `Sports`, `Business`, `Sci/Tech`.\n",
        "\n",
        "Start by downloading the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "urlFMaVtY-Rg"
      },
      "outputs": [],
      "source": [
        "# Download the AG_NEWS dataset\n",
        "agnews_train = AG_NEWS(\"./\", split='train')\n",
        "agnews_test = AG_NEWS(\"./\", split='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pot3Rd_0_nY"
      },
      "source": [
        "To get this dataset into a format that can be diggested by a neural network, we will need to do a bit of work by using the `collate_fn` option of PyTorch's [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
        "\n",
        "Complete the following code to process the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JTwVwJQ4zPO-"
      },
      "outputs": [],
      "source": [
        "# Get a tokenizer from torchtext\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Define how the dataset text and label are going to be pre-processed\n",
        "text_pipeline = lambda x: [glove_vocab[token] for token in tokenizer(x)]  # Use the tokenizer to split the text and find each in the vocabulary\n",
        "label_pipeline = lambda x: int(x) - 1  # Labels for this dataset start at 1\n",
        "\n",
        "# A function to pre-process the text and label\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))                              # Apply label process\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64) # Apply text process\n",
        "         text_list.append(processed_text)\n",
        "\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)                    # Convert labels to tensor\n",
        "    text_list = pad_sequence(sequences=text_list, batch_first=True, padding_value=glove_vocab['<pad>'])  # Ensure all text is padded to the same length\n",
        "\n",
        "    return label_list.to(device), text_list.to(device)\n",
        "\n",
        "# Select a subset of the training dataset to make it more lightweight\n",
        "# and separate into training and validation\n",
        "agnews_train_subset = random.choices(list(iter(agnews_train)), k=int(0.25*len(list(iter(agnews_train)))))\n",
        "num_total = len(agnews_train_subset)\n",
        "num_train = int(num_total * 0.95)\n",
        "split_train, split_valid = random_split(agnews_train_subset, [num_train, num_total - num_train])\n",
        "\n",
        "# Try creating some DataLoaders\n",
        "train_loader = DataLoader(split_train, batch_size=64, num_workers=0, shuffle=True, collate_fn=collate_batch)\n",
        "valid_loader = DataLoader(split_valid, batch_size=1000, num_workers=0, shuffle=False, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(agnews_test, batch_size=1000, num_workers=0, shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiUJhrHY2c9d"
      },
      "source": [
        "### **Embedder class**\n",
        "\n",
        "In order to use GloVe's pre-trained embedding, we will create an `Embedder` class that encapsulates PyTorch's [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).\n",
        "\n",
        "Fill in the embedder class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gMSsMJvDes1i"
      },
      "outputs": [],
      "source": [
        "class Embedder(nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained_embeddings=None, embed_freeze=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Build nn.Embedding from pre-trained embeddings\n",
        "        self.embeddings = nn.Embedding.from_pretrained(\n",
        "            pretrained_embeddings.to(device),  # Make sure to send embeddings to the device\n",
        "            freeze=embed_freeze,   # Ensure the embeddings are not trained\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded_text = self.embeddings(x)\n",
        "        return embedded_text\n",
        "\n",
        "embedder = Embedder(pretrained_embeddings=pretrained_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQZhk76c20vY"
      },
      "source": [
        "We can quickly test the embedder using an example from the training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h0Xmgr7aiI6v",
        "outputId": "38f3830c-0faa-4195-fc28-8614db23f113",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 132, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "example_label, example_text = next(iter(train_loader))\n",
        "\n",
        "embedded_text = embedder(example_text)\n",
        "embedded_text.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(embedded_text[0])"
      ],
      "metadata": {
        "id": "rPs12hdYQkRB",
        "outputId": "ac2abf9e-6680-4b11-8780-9721b6641469",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "132"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iCp7Wb95fL8"
      },
      "source": [
        "## 2. Implement a multi-headed attention layer\n",
        "\n",
        "We are going to manually implement a multi-headed attention layer. Under normal circumstances, you would use PyTorch's predefined layers to built a Transformer, but just this once we will go over the implementation to understand how it works.\n",
        "\n",
        "The self-attention mechanism is based around calculating a key, query, and value vector. These vectors are calculated using matrix multiplications (`nn.Linear`), but we will make the design more efficient by combining the key matrix, query matrix, and value matrix into a single linear layer.\n",
        "\n",
        "Using the template below, implement a self-attention layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odwo5IIFpeCE"
      },
      "outputs": [],
      "source": [
        "class MultiheadedAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads    # Divide dimension across heads\n",
        "        self.scale = head_dim ** -0.5  # Scaling to be used for <q,v> dot product\n",
        "\n",
        "        self.qkv = nn.Linear(in_features=dim, out_features=3*head_dim)                  # 3*dim to account for q, k, v matrices\n",
        "        self.attn_dropout = attention_dropout        # Add dropout\n",
        "\n",
        "        self.projection =          # Add linear layer\n",
        "        self.activation = ##          # And a GELU activation\n",
        "        self.proj_dropout = ##        # Final dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Separate q, v, k matrices\n",
        "        B, N, C = x.shape\n",
        "        qkv = (\n",
        "            self.qkv(x) # B, N, (3*C)\n",
        "            .reshape(B, N, 3, self.num_heads, C // self.num_heads) # B, N, 3(qkv), H(eads), embed_dim\n",
        "            .permute(2, 0, 3, 1, 4)                                # 3, B, H(eads), N, emb_dim\n",
        "            .reshape(3, B*self.num_heads, N, C // self.num_heads)  # 3, B*H(eads), N, emb_dim\n",
        "        )\n",
        "        q, k, v = torch.chunk(qkv, 3)                              # B, H*N, dim\n",
        "        q, k, v = q.squeeze(0), k.squeeze(0), v.squeeze(0)         # Extract matrices\n",
        "\n",
        "        # Calculate the dot product of q and k and softmax the result\n",
        "        # B*H,N,dim x B*H,dim,N -> B*H,N,N\n",
        "        attn = ##  # <q,k> / sqrt(d)\n",
        "        attn = ##  # Softmax over embedding dim\n",
        "        attn = ##  # Dropout\n",
        "\n",
        "        # Multiply the result by v and reshape appropriately\n",
        "        # B*H, N, N\n",
        "        x = (\n",
        "            torch.bmm(attn, v)                                  # B*H,N,N x B*H,N,dim -> B*H,N,dim\n",
        "            .reshape(B, self.num_heads, N, C // self.num_heads) # B, H, N, dim\n",
        "            .transpose(1, 2)                                    # B, N, H, dim\n",
        "            .reshape(B, N, C)                                   # B, N, (H*dim)\n",
        "        )\n",
        "\n",
        "        # Apply a linear operation\n",
        "        x = ##   # Apply linear layer\n",
        "        x = ##   # Apply activation\n",
        "        x = ##   # Apply dropout\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpIQQkLY4OVn"
      },
      "source": [
        "## 3. Build and train a compact transformer\n",
        "\n",
        "Now that we have built our attention layer, we will use it to build a Transformer. In particular, we are going to design it using as inspiration the Compact Transformer architecture introduced in [this paper](https://arxiv.org/pdf/2104.05704).\n",
        "\n",
        "The goal of the Compact Transformer design is to reduce the number of parameters needed to build and train transformers by using a concept called Sequence Pooling.\n",
        "\n",
        "The [accompanying repository](https://github.com/SHI-Labs/Compact-Transformers) contains some code that we have used as inspiration for the implementation below.\n",
        "\n",
        "To build the Compact Transformer we will need a number of building blocks: a `TextEncoder`, a `TransformerEncoderLayer`, and a `TransformerClassifier`.\n",
        "\n",
        "Let's start by building the `TextEncoder`, which will apply a 1D convolution to the output of the embedder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moHVrOqGu5m-"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 n_input_channels=50,\n",
        "                 n_output_channels=768,\n",
        "                 activation=None):\n",
        "\n",
        "        super(TextEncoder, self).__init__()\n",
        "\n",
        "        # Combine a 1D convolution with an optional activation and optional max pooling\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # Use a Conv2D to make a 1D convolution\n",
        "            nn.Conv2d(##,\n",
        "                      kernel_size=##,\n",
        "                      stride=##,\n",
        "                      padding=##, bias=False),\n",
        "            # Activation or identity\n",
        "            ##,\n",
        "        )\n",
        "\n",
        "        # Initialise weights\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = ##  # Unsqueeze dimension 1\n",
        "        x = ##  # Apply conv\n",
        "        x = ##  # Squeeze again\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU2RDcZCwACz"
      },
      "source": [
        "Next, a `TransformerEncoderLayer` will combine our multi-headed attention with a FFN. We will also add a number of training heuristics like [`nn.LayerNorm`](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v8ZJwEkpkvg"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim=768, num_heads=8, dim_feedforward=2048,\n",
        "                 dropout=0.1, attention_dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm_1 = ##  # Layer norm 1\n",
        "        self.norm_2 = ##  # Layer norm 2\n",
        "\n",
        "        # Multi-headed Attention\n",
        "        self.MHA = MultiheadedAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            attention_dropout=attention_dropout,\n",
        "            projection_dropout=dropout,\n",
        "        )\n",
        "\n",
        "        # FFN\n",
        "        self.ff = nn.Sequential(\n",
        "            # Linear layer + GELU activation + Dropout\n",
        "            ##,\n",
        "            ##,\n",
        "            ##,\n",
        "            # Linear layer + Dropout\n",
        "            ##,\n",
        "            ##,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        mha = ##  # Multi-headed attention\n",
        "\n",
        "        x = ##    # Residual connection (Add)\n",
        "        x = ##    # LayerNorm\n",
        "\n",
        "        x2 = ##   # FFN\n",
        "        x = ##    # Residual connection (Add)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNz0B1-4wW3P"
      },
      "source": [
        "We will use a `TransformerClassifier` to concatenate multiple `TransformerEncoderLayer` and add the Sequence Pooling mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSFNWbHEpstp"
      },
      "outputs": [],
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed_dim=768,\t\t    # Embedding dim\n",
        "                 num_layers=4,          # Number of transformer encoders\n",
        "                 num_heads=8,           # Number of attention heads\n",
        "                 attention_dropout=0.1, # Attention Dropout\n",
        "                 dropout=0.1,           # (MHA) Projection Dropout\n",
        "                 mlp_ratio=4,           # (MHA) Projection FF dimension\n",
        "                 num_classes=4,         # Number of classes to classify\n",
        "                 seq_pool=True          # Use sequence pooling?\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        dim_feedforward = int(embed_dim * mlp_ratio)\n",
        "\n",
        "        # Concatenate as many TransformerEncoderLayer as required\n",
        "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderLayer(embed_dim=embed_dim,\n",
        "                                                                           num_heads=num_heads,\n",
        "                                                                           dim_feedforward=dim_feedforward,\n",
        "                                                                           attention_dropout=attention_dropout,\n",
        "                                                                           dropout=dropout)\n",
        "                                                  for _ in range(num_layers)])\n",
        "        self.norm = ##  # Layer norm\n",
        "\n",
        "        # Sequential Pooling weights\n",
        "        self.seq_pool = seq_pool\n",
        "        self.attention_pool = nn.Linear(embed_dim, 1)\n",
        "\n",
        "        # A final projection to the output classes\n",
        "        self.mlp = ##\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional embedding\n",
        "        x += ##\n",
        "\n",
        "        # Apply TransformerEncoderLayer\n",
        "        x = ##\n",
        "        x = ##\n",
        "\n",
        "        # Apply Sequential Pooling\n",
        "        if self.seq_pool:\n",
        "            pool_x = self.attention_pool(x)                                 # Apply linear operation\n",
        "            soft_pool_x = F.softmax(pool_x, dim=1)                          # Calculate softmax\n",
        "            x = torch.matmul(soft_pool_x.transpose(-1, -2), x).squeeze(-2)  # Use the result to weight the information in x\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        # Final MLP\n",
        "        x = ##\n",
        "\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        return pe.unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt4HuvExwu7r"
      },
      "source": [
        "Finally, we can combine the `Embedder` with the `TextEncoder` and the `TransformerClassifier` to build our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFAzrTuusMHd"
      },
      "outputs": [],
      "source": [
        "class TextTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim=768, num_classes=4, patch_size=2,\n",
        "                 vocab=glove_vocab, pretrained_embeddings=pretrained_embeddings, embed_freeze=True,\n",
        "                 **kwargs):\n",
        "        super(TextTransformer, self).__init__()\n",
        "\n",
        "        word_embed_dim = pretrained_embeddings.shape[1]\n",
        "        # Word embedder\n",
        "        self.embedder = Embedder(pretrained_embeddings=pretrained_embeddings, embed_freeze=True)\n",
        "\n",
        "        # Text encoder\n",
        "        self.encoder = TextEncoder(\n",
        "            n_input_channels=word_embed_dim,\n",
        "            n_output_channels=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "            padding=0,\n",
        "            activation=nn.GELU,\n",
        "        )\n",
        "\n",
        "        # Transformer classifier\n",
        "        self.classifier = TransformerClassifier(\n",
        "            embed_dim=embed_dim, num_classes=num_classes,\n",
        "            dropout=0.1,\n",
        "            attention_dropout=0.1,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = ##     # Apply embedder\n",
        "        x = ##     # Apply encoder\n",
        "        out = ##   # Apply classifier\n",
        "        return out\n",
        "\n",
        "example_label, example_text = next(iter(train_loader))\n",
        "\n",
        "model = TextTransformer(\n",
        "    embed_dim=96, num_classes=4, patch_size=2,\n",
        "    num_layers=1, num_heads=8, mlp_ratio=2, seq_pool=True,\n",
        "    vocab=glove_vocab, pretrained_embeddings=pretrained_embeddings\n",
        ")\n",
        "\n",
        "summ = summary(model, input_data=example_text)\n",
        "print(summ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioSh8G3U5vte"
      },
      "source": [
        "### **Define the loss and training loops**\n",
        "\n",
        "It is common when training Transformers to use label smoothing Cross-Entropy as a loss. Label smoothing increases loss when the model is correct and decreases the loss when it is incorrect. This allows us not to punish the model as harshly when labels are incorrect.\n",
        "\n",
        "We are not going to use label smoothing for this training, but we will leave the implementation below for reference. This implementation has been adapted from [this library](https://timm.fast.ai/loss.cross_entropy):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GArLqJ205uDH"
      },
      "outputs": [],
      "source": [
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"\n",
        "    NLL loss with label smoothing.\n",
        "    \"\"\"\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        \"\"\"\n",
        "        Constructor for the LabelSmoothing module.\n",
        "        :param smoothing: label smoothing factor\n",
        "        \"\"\"\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1. - smoothing\n",
        "\n",
        "    def _compute_losses(self, x, target):\n",
        "        log_prob = F.log_softmax(x, dim=-1)\n",
        "        nll_loss = -log_prob.gather(dim=-1, index=target.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -log_prob.mean(dim=-1)\n",
        "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        return self._compute_losses(x, target).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNstdgq4xIyu"
      },
      "source": [
        "We define our train, validate, and test loops as usual:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO9FWeC9iX-H"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, data_loader):\n",
        "    model.train()                         # the model is in the training mode so the parameters(weights)to be optimised will be updated\n",
        "    train_loss, train_accuracy = 0, 0     # initialise loss and accuracy to 0 for training\n",
        "    for y, X in data_loader:              # iterate over the mini-batches defined in the data loader\n",
        "        X, y = X.to(device), y.to(device) # send data to the device (GPU in our case)\n",
        "        optimizer.zero_grad()             # resetting optimiser info\n",
        "        y_hat = model(X)                  # forward pass\n",
        "        loss = criterion(y_hat, y)        # compute loss\n",
        "        loss.backward()                   # backpropagation to calculate the gradients\n",
        "        train_loss += loss*X.size(0)      # # add it up for different mini-batches and undo loss normalisation\n",
        "        y_pred = F.log_softmax(y_hat, dim=1).max(1)[1]  # get predictions\n",
        "        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0) # compute accuracy\n",
        "        optimizer.step()                  # perform a step of gradient descent\n",
        "\n",
        "    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)  # here we can average over the whole dataset\n",
        "\n",
        "\n",
        "def validate(model, criterion, data_loader):      # does not need optimiser\n",
        "    model.eval()                                  # model is set to evaluation mode so no dropout or any other funny stuff here\n",
        "    validation_loss, validation_accuracy = 0., 0. # initialise loss and accuracy to 0 for training\n",
        "    for y, X in data_loader:                      # iterate over the mini-batches defined in the data loader\n",
        "        with torch.no_grad():                     # deactivates autograd engine\n",
        "            X, y = X.to(device), y.to(device)     # send data to the device (GPU in our case)\n",
        "            y_hat = model(X)                      # forward pass\n",
        "            loss = criterion(y_hat, y)            # evaluate loss\n",
        "            validation_loss += loss*X.size(0)     # add it up for different mini-batches and undo loss normalisation\n",
        "            y_pred = F.log_softmax(y_hat, dim=1).max(1)[1]  # get predictions\n",
        "            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0) # compute accuracy\n",
        "\n",
        "    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)  # here we can average over the whole dataset\n",
        "\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    ys, y_preds = [], []\n",
        "    for y, X in data_loader:\n",
        "        with torch.no_grad():\n",
        "            X, y = X.to(device), y.to(device) # data and labels to device\n",
        "            y_hat = model(X)                  # forward pass and reshape tensor and get it ready to the fully connected layer\n",
        "            y_pred = F.log_softmax(y_hat, dim=1).max(1)[1] # calculate prediction\n",
        "            ys.append(y.cpu().numpy())        # save predictions\n",
        "            y_preds.append(y_pred.cpu().numpy()) # save predictions\n",
        "\n",
        "    return np.concatenate(y_preds, 0),  np.concatenate(ys, 0) ## concatenate the labels of each batch into a single list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id4cAITnxOSW"
      },
      "source": [
        "Let's define some hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCB-LxInjUb8"
      },
      "outputs": [],
      "source": [
        "seed = 42 ## keep at that if you want to get almost the exact same results (down to numerical precision I guess)\n",
        "lr = 1e-2\n",
        "batch_size = 64\n",
        "test_batch_size = 1000\n",
        "n_epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRBBILdwxWm3"
      },
      "source": [
        "And we can finally train the model! Once you have trained the model, answer the following questions:\n",
        "\n",
        "1. What is the impact of the sequence pooling on the training?\n",
        "2. What is the impact of the number of heads and layers in our architecture?\n",
        "3. What is the impact of using a different embedding?\n",
        "4. How could you improve the network's performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92vklyLkjL_5"
      },
      "outputs": [],
      "source": [
        "# Create model and send to device\n",
        "model = TextTransformer(\n",
        "    embed_dim=96, num_classes=4, patch_size=2,\n",
        "    num_layers=1, num_heads=8, mlp_ratio=2, seq_pool=True,\n",
        "    vocab=glove_vocab, pretrained_embeddings=pretrained_embeddings\n",
        ").to(device)\n",
        "\n",
        "set_seed(seed)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)   # instantiate the optimizer\n",
        "criterion = nn.CrossEntropyLoss()                         # instantiate the criterion\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(split_train, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=collate_batch)\n",
        "valid_loader = DataLoader(split_valid, batch_size=test_batch_size, num_workers=0, shuffle=False, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(agnews_test, batch_size=test_batch_size, num_workers=0, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "liveloss = PlotLosses()    # plots evolution of loss and accuracy\n",
        "for epoch in range(n_epochs):\n",
        "    logs = {}\n",
        "    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
        "\n",
        "    logs['' + 'log loss'] = train_loss.item()\n",
        "    logs['' + 'accuracy'] = train_accuracy\n",
        "\n",
        "    validation_loss, validation_accuracy = validate(model, criterion, valid_loader)\n",
        "    logs['val_' + 'log loss'] = validation_loss.item()\n",
        "    logs['val_' + 'accuracy'] = validation_accuracy\n",
        "\n",
        "    liveloss.update(logs)\n",
        "    liveloss.draw()\n",
        "    print(validation_loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR3ncx7otXXg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}